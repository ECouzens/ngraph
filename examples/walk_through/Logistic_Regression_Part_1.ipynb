{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "-------------------\n",
    "\n",
    "This example performs logistic regression. The corresponding jupyter notebook is found [here](https://github.com/NervanaSystems/ngraph/blob/master/examples/walk_through/Logistic_Regression_Part_1.ipynb).\n",
    "We want to classify an observation $x$ into one of two classes, denoted by $y=0$ and $y=1$. Using a simple linear model:\n",
    "$$\\hat{y}=\\sigma(Wx)$$\n",
    "\n",
    "we want to find the optimal values for $W$. Here, we use gradient descent with **a learning rate of $\\alpha$** and the cross-entropy as the error function.\n",
    "\n",
    "### Axes\n",
    "\n",
    "The nervana graph uses `Axes` to attach shape information to tensors. The identity of `Axis` objects are used to pair and specify dimensions in symbolic expressions. The function ``ng.make_axis`` will create an ``Axis`` object with an optionally supplied `name` argument. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ngraph as ng\n",
    "import ngraph.transformers as ngt\n",
    "    \n",
    "my_axis = ng.make_axis(length=256, name='my_axis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use a ``NameScope`` to set the names of the various axes. A ``NameScope`` is an object that sets the name of an object to that of its assigned attribute. So when we set ``ax.N`` to an ``Axis`` object, the ``name`` of the object is automatically set to ``ax.N``. This a convenient way to define axes, so we use this approach for the rest of this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = ng.make_name_scope(\"ax\")\n",
    "ax.N = ng.make_axis(length=128, batch=True)\n",
    "ax.C = ng.make_axis(length=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add ``batch`` as a property to ``ax.N`` to indicate that the axis is a batch axis. **A batch axis is held out of the default set of axes reduced in reduction operations such as sums.**\n",
    "\n",
    "### Building the graph\n",
    "Our model has three placeholders: ``X``, ``Y``, and ``alpha``, each of which need to have axes defined. ``alpha`` is a scalar, so we pass in empty axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = ng.placeholder(axes=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``X`` and ``Y`` are tensors for the input and output data, respectively. **Our convention is to use the last axis for samples**.  The placeholders can be specified as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = ng.placeholder(axes=[ax.C, ax.N])\n",
    "Y = ng.placeholder(axes=[ax.N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify the training weights, ``W``.  **Unlike a placeholder, ``W`` should retain its value from computation to computation (for example, across mini-batches of training).**  Following TensorFlow, we call this a *variable*.  We specify the variable with both ``Axes`` and also an initial value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = ng.variable(axes=[ax.C - 1], initial_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nervana graph axes are agnostic to data layout on the compute device, so the ordering of the axes does not matter. As a consequence, **when two tensors are provided to a `ng.dot()` operation**, for example, ***one needs to indicate which are the corresponding axes that should be matched together.*** We use \"dual offsets\" of +/- 1 to mark which axes should be matched during a multi-axis operation, which gives rise to the `ax.C - 1` observed above. Every axis is a member of a family of axes we call duals of the axis, and each axis in the family has a position. ***When you create an axis, its dual position is 0***. When ``ng.dot`` is called, **it pairs axes in the first and second arguments** that are **_of the same dual family_** and have **consecutive positions**. For more information, see the `Axes` section of the user guide [here](https://ngraph.nervanasys.com/docs/latest/axes.html)\n",
    "\n",
    "Now we can estimate ``y`` as ``Y_hat`` and compute the average loss ``L``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = ng.sigmoid(ng.dot(W, X))\n",
    "L = ng.cross_entropy_binary(Y_hat, Y, out_axes=()) / ng.batch_size(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use several ngraph functions, including ``ng.dot`` and ``ng.sigmoid``. Since a tensor can have multiple axes, we need a way to mark which axes in the first argument of ``ng.dot`` are to act on which axes in the second argument. Please also note that the `W` has been defined with one axis, while `X` has two axis. Every tensor component along C axis in `X` is being dot-producted with `W`, and the `N` results are stored in `Y_hat`, that has only one axis, the `N` axis.\n",
    "\n",
    "As already noted, when ``ng.dot`` is called, **it pairs axes in the first and second arguments that are of the same dual family** and have **consecutive positions**. In what shown above, the dual position of the `X` axis of `W` was \"decreased\" to `-1`, being `0` the default position assigned when `W` was created. Remember that we want the variable `W` to act on the `ax.C` axis of the input `X`, so we want the axis for `W` to be in the position before `ax.C`, which we can obtain with `ax.C - 1`. \n",
    "\n",
    "Once `Y_hat` has been computed (the whole batch computation was defined above), we can move on and correct the weights in `W`. Gradient descent requires computing the gradient, $\\frac{dL}{dW}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grad = ng.deriv(L, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``ng.deriv`` function computes the backprop using **autodiff**. We are almost done as we are now ready to update ``W``.  The update step (which is an Op that will be carried out at the time of real computation on the device) computes the new weight and assigns it to ``W``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ngraph.op_graph.op_graph.AssignOp"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update = ng.assign(W, W - alpha * grad / ng.tensor_size(Y_hat))\n",
    "type(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "**Now we create a transformer and define a computation for learning**. In order to do so, we pass the ops from which we want to retrieve the results for, followed by the placeholders:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer = ngt.make_transformer()\n",
    "update_fun = transformer.computation([L, W, update], alpha, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the **computation will return three values** for the** ``L``, ``W``, and ``update``**, given inputs to fill the placeholders,** $\\alpha$ (LR), X (inputs), Y (expected outputs).**\n",
    "\n",
    "Therefore to run the computation we need to generate input data. Here below the input data, ``X`` and ``Y``, is synthetically generated as a mixture of two Gaussian distributions in 4-d space.  We shape our entire dataset as 10 mini-batches of 128 samples each, which we create with a convenient function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gendata\n",
    "\n",
    "g = gendata.MixtureGenerator([.5, .5], (ax.C.length,))\n",
    "XS, YS = g.gen_data(ax.N.length, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the model across the 10 epochs, printing the loss and updated weights. Please note that we are using a decreasing policy (with the epoch number) for $\\alpha$. Also note that there is no need to invoke update_fun specifying the outputs, as they were specified at definition time. Now we need only to feed the inputs into ``update_fun``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [ 2.26889729  5.24611712  0.55897683  1.15695345], loss 0.174076288939\n",
      "W: [ 2.31940055  5.31047344  0.5742268   1.17916524], loss 0.135152488947\n",
      "W: [ 2.36883974  5.38630724  0.53643304  1.14578247], loss 0.162494510412\n",
      "W: [ 2.38650537  5.43609238  0.52788138  1.16376483], loss 0.186021730304\n",
      "W: [ 2.41335869  5.49430609  0.49320835  1.23979235], loss 0.166057318449\n",
      "W: [ 2.45394778  5.54222155  0.47551811  1.21743679], loss 0.154901966453\n",
      "W: [ 2.48280835  5.52348232  0.3891854   1.28643167], loss 0.244514852762\n",
      "W: [ 2.5384531   5.54951572  0.42466164  1.42140508], loss 0.172828257084\n",
      "W: [ 2.55841827  5.58289671  0.38803348  1.44254649], loss 0.197061210871\n",
      "W: [ 2.63447213  5.58814955  0.41302529  1.52762759], loss 0.19623811543\n",
      "W: [ 2.63638568  5.62076712  0.40745494  1.49877357], loss 0.170830905437\n",
      "W: [ 2.65651441  5.65096331  0.41057384  1.49799263], loss 0.12679502368\n",
      "W: [ 2.67850089  5.68695545  0.38931185  1.47049344], loss 0.155834704638\n",
      "W: [ 2.68363738  5.71074533  0.38148576  1.46745241], loss 0.181505739689\n",
      "W: [ 2.69346881  5.74043322  0.35921052  1.49296606], loss 0.156439602375\n",
      "W: [ 2.71110249  5.76485491  0.34984383  1.47131443], loss 0.151575654745\n",
      "W: [ 2.72318244  5.75618076  0.30429772  1.49704242], loss 0.238673016429\n",
      "W: [ 2.7479794   5.76844025  0.31961867  1.55608797], loss 0.164360538125\n",
      "W: [ 2.75628734  5.7826395   0.30075303  1.56324506], loss 0.193687498569\n",
      "W: [ 2.792099    5.7838788   0.31203538  1.60300064], loss 0.191592559218\n",
      "W: [ 2.79304528  5.80308962  0.3088989   1.58470964], loss 0.168964534998\n",
      "W: [ 2.80583096  5.82117176  0.31136006  1.58430338], loss 0.123633787036\n",
      "W: [ 2.82018685  5.84294987  0.29828116  1.56651723], loss 0.152323544025\n",
      "W: [ 2.82305813  5.85773563  0.29285228  1.56305313], loss 0.179672509432\n",
      "W: [ 2.82924676  5.87632704  0.27817297  1.5792166 ], loss 0.152478337288\n",
      "W: [ 2.84044075  5.89163399  0.27298054  1.56398833], loss 0.149843305349\n",
      "W: [ 2.84812164  5.88508129  0.24273324  1.58016741], loss 0.236207902431\n",
      "W: [ 2.86400437  5.89242697  0.2528812   1.61810899], loss 0.160994470119\n",
      "W: [ 2.86910534  5.90080214  0.24045727  1.62233555], loss 0.191986203194\n",
      "W: [ 2.8923974   5.90082979  0.24788901  1.6484406 ], loss 0.1893042624\n",
      "W: [ 2.89295912  5.91419411  0.24578369  1.63509154], loss 0.167992457747\n",
      "W: [ 2.90227437  5.92686987  0.24780917  1.63487041], loss 0.121821060777\n",
      "W: [ 2.91290998  5.94219208  0.2385399   1.62186921], loss 0.150207698345\n",
      "W: [ 2.91482568  5.9527669   0.2344052   1.61872625], loss 0.178592830896\n",
      "W: [ 2.91933346  5.96607637  0.22356835  1.63064694], loss 0.150060594082\n",
      "W: [ 2.92748165  5.9770298   0.22026761  1.6190238 ], loss 0.148758992553\n",
      "W: [ 2.93309808  5.97164536  0.19772537  1.63089406], loss 0.234681680799\n",
      "W: [ 2.94473481  5.97669506  0.20539361  1.65881944], loss 0.158959686756\n",
      "W: [ 2.94835496  5.9824338   0.19617812  1.66176188], loss 0.190889239311\n",
      "W: [ 2.96554756  5.98204422  0.20174204  1.68118608], loss 0.187799870968\n",
      "W: [ 2.96591091  5.99219799  0.20018415  1.67066622], loss 0.167388677597\n",
      "W: [ 2.973207    6.00187016  0.20189394  1.67051792], loss 0.120592236519\n",
      "W: [ 2.98163867  6.01357794  0.19477987  1.66030192], loss 0.148742243648\n",
      "W: [ 2.98303628  6.02175093  0.19143938  1.65749729], loss 0.177855327725\n",
      "W: [ 2.9865706   6.03202963  0.18288144  1.66695321], loss 0.148357093334\n",
      "W: [ 2.99295044  6.04048347  0.1806028   1.65756679], loss 0.147996842861\n",
      "W: [ 2.99736714  6.03588295  0.16267121  1.66695869], loss 0.233595445752\n",
      "W: [ 3.00652528  6.03964424  0.16886151  1.68902791], loss 0.157530993223\n",
      "W: [ 3.00930166  6.04392195  0.16155244  1.69124997], loss 0.190097630024\n",
      "W: [ 3.02289176  6.04337358  0.16600348  1.70669937], loss 0.186690345407\n",
      "W: [ 3.02313828  6.05151606  0.16477938  1.69801021], loss 0.166972011328\n",
      "W: [ 3.02911711  6.05929422  0.16625409  1.6978929 ], loss 0.11968010664\n",
      "W: [ 3.03609228  6.06871271  0.16051368  1.68948936], loss 0.147640272975\n",
      "W: [ 3.03716922  6.07534504  0.15770882  1.68697107], loss 0.177307516336\n",
      "W: [ 3.04006863  6.08367538  0.15065086  1.6948086 ], loss 0.147058844566\n",
      "W: [ 3.04529691  6.09052324  0.14899205  1.68693578], loss 0.147421374917\n",
      "W: [ 3.04892993  6.08649445  0.13412115  1.69471145], loss 0.232761442661\n",
      "W: [ 3.05646658  6.08944654  0.13932361  1.71293926], loss 0.156444609165\n",
      "W: [ 3.05870199  6.09281158  0.13327393  1.71470284], loss 0.189487248659\n",
      "W: [ 3.06991792  6.09220505  0.13698392  1.72751546], loss 0.18581789732\n",
      "W: [ 3.07008934  6.09897661  0.13598248  1.72010839], loss 0.166664227843\n",
      "W: [ 3.07514334  6.10545826  0.13727693  1.72000539], loss 0.118963494897\n",
      "W: [ 3.08108568  6.11330605  0.13248357  1.71287251], loss 0.146766632795\n",
      "W: [ 3.08194733  6.11887074  0.13006432  1.71059024], loss 0.176878228784\n",
      "W: [ 3.08440065  6.12584925  0.12406567  1.71728194], loss 0.146019071341\n",
      "W: [ 3.08882141  6.13158369  0.12281344  1.71049964], loss 0.146965682507\n",
      "W: [ 3.09190273  6.12799358  0.11011939  1.71713579], loss 0.232089549303\n",
      "W: [ 3.0982976   6.13039684  0.11461247  1.732651  ], loss 0.155576258898\n",
      "W: [ 3.10015845  6.13314438  0.10945532  1.73409903], loss 0.18899551034\n",
      "W: [ 3.10969377  6.13252354  0.11263574  1.74503505], loss 0.185102894902\n",
      "W: [ 3.10981441  6.13830376  0.11179247  1.73857653], loss 0.166425943375\n",
      "W: [ 3.11418438  6.14384508  0.11294493  1.73848057], loss 0.118378303945\n",
      "W: [ 3.11935663  6.15055227  0.10884162  1.73228705], loss 0.146048128605\n",
      "W: [ 3.12006521  6.1553359   0.10671338  1.73020017], loss 0.176528990269\n",
      "W: [ 3.12218809  6.16132593  0.10150147  1.73603737], loss 0.145157173276\n",
      "W: [ 3.12601185  6.16624689  0.10053143  1.73007762], loss 0.146592274308\n",
      "W: [ 3.12868452  6.16300488  0.08946354  1.73586643], loss 0.231529951096\n",
      "W: [ 3.13423276  6.16501474  0.09342135  1.74936485], loss 0.154857844114\n",
      "W: [ 3.13582015  6.16731977  0.0889292   1.75058365], loss 0.188586756587\n",
      "W: [ 3.14410496  6.16670465  0.09171205  1.76011646], loss 0.184499680996\n",
      "W: [ 3.1441896   6.17173719  0.0909864   1.75438869], loss 0.166234999895\n",
      "W: [ 3.1480341   6.17656755  0.09202439  1.75429654], loss 0.117886781693\n",
      "W: [ 3.1526103   6.18241119  0.08844494  1.74882519], loss 0.145441174507\n",
      "W: [ 3.15320516  6.18659973  0.08654422  1.74690211], loss 0.176236957312\n",
      "W: [ 3.15507364  6.19183683  0.0819391   1.75207746], loss 0.144424527884\n",
      "W: [ 3.15843892  6.19613838  0.0811734   1.74676037], loss 0.146278426051\n",
      "W: [ 3.16079664  6.19317961  0.07136576  1.75189412], loss 0.231052339077\n",
      "W: [ 3.16569257  6.19489527  0.07490475  1.76383495], loss 0.154248327017\n",
      "W: [ 3.1670723   6.19687033  0.07092688  1.76488042], loss 0.188238993287\n",
      "W: [ 3.17439079  6.19627047  0.07340022  1.77332485], loss 0.183979660273\n",
      "W: [ 3.17444897  6.20071983  0.07276518  1.76817739], loss 0.166077971458\n",
      "W: [ 3.17787743  6.20499468  0.07370906  1.76808751], loss 0.11746506393\n",
      "W: [ 3.18197918  6.21016359  0.07054009  1.76318848], loss 0.144917815924\n",
      "W: [ 3.18248677  6.21388435  0.06882221  1.76140428], loss 0.175987541676\n",
      "W: [ 3.1841538   6.2185297   0.064699    1.76605201], loss 0.14378964901\n",
      "W: [ 3.18715644  6.22234488  0.06408622  1.76125073], loss 0.146009385586\n",
      "W: [ 3.18926406  6.21962166  0.05528351  1.7658627 ], loss 0.230637013912\n",
      "W: [ 3.19364238  6.22111082  0.0584855   1.77656472], loss 0.153721183538\n",
      "W: [ 3.19485927  6.22283077  0.05491707  1.77747512], loss 0.187937676907\n",
      "W: [ 3.2014091   6.22225046  0.05714259  1.78505099], loss 0.183523863554\n"
     ]
    }
   ],
   "source": [
    "    for i in range(10):\n",
    "        for xs, ys in zip(XS, YS):\n",
    "            loss_val, w_val, _ = update_fun(5.0 / (1 + i), xs, ys)\n",
    "            print(\"W: %s, loss %s\" % (w_val, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see Part 2 of logistic regressions, which walks uses through adding additional variables, computations, and dimensions. <br>  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
